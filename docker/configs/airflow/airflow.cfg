[core]
# Executor settings
executor = LocalExecutor
dags_are_paused_at_creation = False
load_examples = False

# Database connection string
sql_alchemy_conn = postgresql+psycopg2://predictestateuser:securepassword@postgres:5432/predictestate
sql_alchemy_pool_size = 5
sql_alchemy_max_overflow = 10
parallelism = 32
max_active_tasks_per_dag = 16
max_active_runs_per_dag = 2
default_timezone = utc

# File path for DAGs
dags_folder = /opt/airflow/dags

# Logging
base_log_folder = /opt/airflow/logs
remote_logging = False

# Hostname and port
hostname_callable = socket:getfqdn
default_webserver_hostname = 0.0.0.0
default_webserver_port = 8080

# Other core settings
default_task_retries = 1
default_task_retry_delay = 300

[webserver]
# Webserver configuration
web_server_host = 0.0.0.0
web_server_port = 8080
base_url = http://localhost:8080
web_server_worker_timeout = 120

[logging]
# Logging configuration
base_log_folder = /opt/airflow/logs
remote_logging = False
logging_level = INFO
log_format = [%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s
simple_log_format = %(asctime)s %(levelname)s - %(message)s

[scheduler]
# Scheduler configuration
scheduler_heartbeat_sec = 5
max_threads = 2
job_heartbeat_sec = 5
statsd_on = False

[worker]
# Celery worker configuration (not used for LocalExecutor)
worker_concurrency = 16

[secrets]
# Secrets backend
backend = airflow.providers.hashicorp.secrets.vault.VaultBackend
backend_kwargs = {"connections_path": "airflow/connections", "variables_path": "airflow/variables"}

[metrics]
# Metrics reporting
statsd_on = False

[elasticsearch]
# Elasticsearch logging
elasticsearch_host = http://elasticsearch:9200
elasticsearch_log_id_template = {dag_id}-{task_id}-{execution_date}-{try_number}

[kubernetes]
# Kubernetes executor
worker_container_repository = airflow-worker
worker_container_tag = latest
namespace = airflow
